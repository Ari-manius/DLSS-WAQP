{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8517b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "from torch_geometric.nn import GATConv\n",
    "#from utils.create_split_masks import create_split_masks_regression\n",
    "#from utils.earlyStopping import EarlyStopping\n",
    "#from utils.train_GNN_model import train_GNN_model\n",
    "#from utils.initialize_weights import initialize_weights\n",
    "import argparse\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d82ade32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split_masks_regression(data, train_ratio=0.7, val_ratio=0.1, seed=42, normalize=True):\n",
    "    torch.manual_seed(seed)\n",
    "    n = data.num_nodes\n",
    "    indices = torch.randperm(n)\n",
    "    \n",
    "    n_train = int(n * train_ratio)\n",
    "    n_val = int(n * val_ratio)\n",
    "    \n",
    "    train_idx = indices[:n_train]\n",
    "    val_idx = indices[n_train:n_train + n_val]\n",
    "    test_idx = indices[n_train + n_val:]\n",
    "\n",
    "    if normalize:\n",
    "        X = data.x\n",
    "        scaler = StandardScaler()\n",
    "        X_train = X[train_idx].numpy()\n",
    "        scaler.fit(X_train)\n",
    "        data.x = torch.tensor(scaler.transform(X.numpy()), dtype=torch.float32)\n",
    "\n",
    "    train_mask = torch.zeros(n, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(n, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(n, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "    \n",
    "    return train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "292e1143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (conv1): GATConv(39, 64, heads=8)\n",
      "  (lin): Linear(in_features=512, out_features=64, bias=True)\n",
      "  (conv2): GATConv(64, 3, heads=1)\n",
      "  (elu): ELU(alpha=1.0)\n",
      "  (dropout): Dropout(p=0.6, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#data_path = \"C:/Users/Nutzer/Desktop/DLSS/final_project/DLSS-WAQP/2_GNN_Models/data/data_minmax_catagg.pt\"\n",
    "data_path = \"data/data_quantile_Target_QC_aggcat.pt\"\n",
    "data_file = \"data_quantile_Target_QC_aggcat.pt\"\n",
    "\n",
    "\n",
    "data = torch.load(data_path, weights_only=False)\n",
    "train_mask, val_mask, test_mask = create_split_masks_regression(data)\n",
    "data.y = data.y.view(-1).long()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_dim = data.num_node_features\n",
    "hidden_dim = 64\n",
    "# number of attention heads for the first layer\n",
    "heads = 8\n",
    "# output_dim equals number of classes\n",
    "output_dim = len(data.y.unique())\n",
    "\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim, heads=8, dropout=0.6):\n",
    "        super(GAT, self).__init__()\n",
    "        # Graph Attention layer: in -> hid*heads\n",
    "        self.conv1 = GATConv(in_dim, hid_dim, heads=heads, dropout=dropout)\n",
    "        # Linear map to combine the heads output\n",
    "        self.lin = nn.Linear(hid_dim * heads, hid_dim)\n",
    "        # final GAT layer: hid -> out\n",
    "        self.conv2 = GATConv(hid_dim, out_dim, heads=1, concat=False, dropout=dropout)\n",
    "        self.elu = nn.ELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        # First attention layer\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.elu(x)\n",
    "        # Combine multi-head features\n",
    "        x = self.lin(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        # Output attention layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Instantiate model\n",
    "model = GAT(input_dim, hidden_dim, output_dim, heads=heads).to(device)\n",
    "print(model)\n",
    "\n",
    "# Assign masks\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "# Initialize weights and compile\n",
    "#initialize_weights(model)\n",
    "#model = torch.compile(model)\n",
    "# InductorError: RuntimeError: Compiler: cl is not found. \n",
    "# このエラーが出るのはmodelのコードが原因だからコメントアウトするだけでオッケー\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc19e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# Residual GAT Model\n",
    "# -------------------------------\n",
    "class ResidualGAT(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved Graph Attention Network with residual connections,\n",
    "    LayerNorm, GELU activation and dropout tuning.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hid_dim, out_dim, heads=8, num_layers=3, dropout=0.4):\n",
    "        super(ResidualGAT, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "\n",
    "        # First layer\n",
    "        self.convs.append(GATConv(in_dim, hid_dim, heads=heads, dropout=dropout))\n",
    "        self.norms.append(nn.LayerNorm(hid_dim * heads))\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(num_layers - 2):\n",
    "            self.convs.append(GATConv(hid_dim * heads, hid_dim, heads=heads, dropout=dropout))\n",
    "            self.norms.append(nn.LayerNorm(hid_dim * heads))\n",
    "\n",
    "        # Final layer\n",
    "        self.convs.append(GATConv(hid_dim * heads, out_dim, heads=1, concat=False, dropout=dropout))\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        for i in range(len(self.convs) - 1):\n",
    "            identity = x\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.norms[i](x)\n",
    "            x = self.gelu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            # Residual connection\n",
    "            if identity.shape == x.shape:\n",
    "                x = x + identity\n",
    "\n",
    "        # Output layer (no residual here)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Training function\n",
    "# -------------------------------\n",
    "def train_GNN_model(epochs, model, optimizer, criterion, data, early_stopper, scheduler, device):\n",
    "    train_losses, val_losses = [], []\n",
    "    data = data.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(data)\n",
    "            val_loss = criterion(val_out[data.val_mask], data.y[data.val_mask]).item()\n",
    "        model.train()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # early stopping\n",
    "        if early_stopper(val_loss, model):\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Train Loss = {loss.item():.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Early Stopping Utility\n",
    "# -------------------------------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=30, min_delta=0.0001, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "            return False\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Main Training Script\n",
    "# -------------------------------\n",
    "# data, input_dim, hidden_dim, output_dim は事前に定義されている想定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResidualGAT(input_dim, hidden_dim, output_dim, heads=8, num_layers=3).to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.005, weight_decay=1e-3)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.005,\n",
    "    steps_per_epoch=1,   \n",
    "    epochs=300,\n",
    "    pct_start=0.1,\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=10,\n",
    "    final_div_factor=1e4\n",
    ")\n",
    "\n",
    "labels = data.y\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "early_stopper = EarlyStopping(patience=30, min_delta=0.0001, path=\"ResidualGAT_checkpoint.pt\")\n",
    "\n",
    "train_losses, val_losses = train_GNN_model(\n",
    "    epochs=300,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    data=data,\n",
    "    early_stopper=early_stopper,\n",
    "    scheduler=scheduler,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Plot Loss Curve\n",
    "# -------------------------------\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
