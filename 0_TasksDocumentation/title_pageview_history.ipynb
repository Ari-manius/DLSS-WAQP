{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c93d61f-6795-47af-8e27-5d7a5f7a46e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pageid\n",
      "0  18955875\n",
      "1    682482\n",
      "2     24544\n",
      "3     32927\n",
      "4    325329\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Load only pageid column from your dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\nafis\\Downloads\\wikipedia_articles_cleaned.csv\", usecols=['pageid'])\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cd010e7-8428-42e1-abde-04d7750be049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch titles for page IDs\n",
    "def get_titles_from_ids(page_ids):\n",
    "    titles = {}\n",
    "    for i in range(0, len(page_ids), 50):  # API allows up to 50 IDs per request\n",
    "        batch = page_ids[i:i+50]\n",
    "        ids_str = \"|\".join(str(pid) for pid in batch)\n",
    "        url = f\"https://en.wikipedia.org/w/api.php?action=query&pageids={ids_str}&format=json\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            for page in data['query']['pages'].values():\n",
    "                titles[page['pageid']] = page['title']\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} for IDs {ids_str}\")\n",
    "        time.sleep(0.2)  # Avoid API rate limits\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9deffd6a-a722-423e-a8c4-e3acbc80540d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pageid              title\n",
      "0     655             Abacus\n",
      "1    1271  Analytical engine\n",
      "2    1372              Amber\n",
      "3    2027       Andrew Wiles\n",
      "4    5180          Chemistry\n"
     ]
    }
   ],
   "source": [
    "# Get titles for all page IDs\n",
    "page_ids = df['pageid'].tolist()\n",
    "titles_dict = get_titles_from_ids(page_ids)\n",
    "\n",
    "# Create a new DataFrame with pageid and title only\n",
    "titles_df = pd.DataFrame(list(titles_dict.items()), columns=['pageid', 'title'])\n",
    "\n",
    "# Preview the result\n",
    "print(titles_df.head())\n",
    "\n",
    "# Save to CSV (optional)\n",
    "titles_df.to_csv(r\"C:\\Users\\nafis\\Downloads\\pageid_title.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc63c6d2-265d-41e7-bb04-a1f2eea94959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "# Load your pageid-title file\n",
    "titles_df = pd.read_csv(r\"C:\\Users\\nafis\\Downloads\\pageid_title.csv\")\n",
    "\n",
    "# Function to get total pageviews for a title\n",
    "def get_pageviews(title):\n",
    "    try:\n",
    "        # URL-encode the title for API\n",
    "        title_url = quote(title.replace(\" \", \"_\"))\n",
    "        url = f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/user/{title_url}/monthly/20230101/20231231\"\n",
    "        headers = {\n",
    "            \"User-Agent\": \"WikipediaQualityBot/1.0 (your_email@example.com)\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            total_views = sum(item['views'] for item in data['items'])\n",
    "            return total_views\n",
    "        else:\n",
    "            print(f\"Failed for {title}: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error for {title}: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8ebf5ca-5b29-4a0d-b258-f749c472c377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Collecting pageviews for first 100 articles...\n",
      "Failed for Amy Roth McDuffie: 404\n",
      "Failed for AN/FSQ-7 Combat Direction Central: 404\n",
      "✅ Pageviews for first 100 articles saved to pageid_title_sample.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafis\\AppData\\Local\\Temp\\ipykernel_22812\\4143210503.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  titles_sample['pageviews'] = titles_sample['title'].apply(get_pageviews)\n"
     ]
    }
   ],
   "source": [
    "# Limit to first 100 articles for testing\n",
    "titles_sample = titles_df.head(100)\n",
    "\n",
    "print(\"⏳ Collecting pageviews for first 100 articles...\")\n",
    "titles_sample['pageviews'] = titles_sample['title'].apply(get_pageviews)\n",
    "\n",
    "# Save test result to a new CSV\n",
    "titles_sample.to_csv(r\"C:\\Users\\nafis\\Downloads\\pageid_title_sample.csv\", index=False)\n",
    "print(\"✅ Pageviews for first 100 articles saved to pageid_title_sample.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a7580f5-2a45-44f1-9cdf-a7ccec9e277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "\n",
    "# Load the sample CSV with first 100 articles\n",
    "titles_sample = pd.read_csv(r\"C:\\Users\\nafis\\Downloads\\pageid_title_sample.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65343366-dd5e-43a0-bc24-28681e4ef610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get number of edits\n",
    "def get_edit_count(title):\n",
    "    try:\n",
    "        title_url = quote(title.replace(\" \", \"_\"))\n",
    "        url = f\"https://en.wikipedia.org/w/api.php?action=query&titles={title_url}&prop=revisions&rvprop=timestamp&rvlimit=500&format=json\"\n",
    "        headers = {\n",
    "            \"User-Agent\": \"WikipediaQualityBot/1.0 (your_email@example.com)\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            pages = data['query']['pages']\n",
    "            page = next(iter(pages.values()))\n",
    "            revisions = page.get('revisions', [])\n",
    "            return len(revisions)  # Number of revisions found (max 500)\n",
    "        else:\n",
    "            print(f\"Failed for {title}: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error for {title}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90701508-ed54-42ab-a55d-22f0cec3bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get number of unique editors\n",
    "def get_num_editors(title):\n",
    "    try:\n",
    "        title_url = quote(title.replace(\" \", \"_\"))\n",
    "        url = (f\"https://en.wikipedia.org/w/api.php?action=query\"\n",
    "               f\"&titles={title_url}&prop=revisions&rvprop=user\"\n",
    "               f\"&rvlimit=500&format=json\")\n",
    "        headers = {\n",
    "            \"User-Agent\": \"WikipediaQualityBot/1.0 (your_email@example.com)\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            pages = data['query']['pages']\n",
    "            page = next(iter(pages.values()))\n",
    "            revisions = page.get('revisions', [])\n",
    "            editors = {rev['user'] for rev in revisions}  # Unique editors\n",
    "            return len(editors)\n",
    "        else:\n",
    "            print(f\"Failed for {title}: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error for {title}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6667534-e7cc-4de7-bf6b-2e6c55832410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get 3 most recent edit timestamps\n",
    "def get_last_3_edits(title):\n",
    "    try:\n",
    "        title_url = quote(title.replace(\" \", \"_\"))\n",
    "        url = (f\"https://en.wikipedia.org/w/api.php?action=query\"\n",
    "               f\"&titles={title_url}&prop=revisions&rvprop=timestamp\"\n",
    "               f\"&rvlimit=3&rvdir=older&format=json\")\n",
    "        headers = {\n",
    "            \"User-Agent\": \"WikipediaQualityBot/1.0 (your_email@example.com)\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            pages = data['query']['pages']\n",
    "            page = next(iter(pages.values()))\n",
    "            revisions = page.get('revisions', [])\n",
    "            timestamps = [rev['timestamp'] for rev in revisions]\n",
    "            return timestamps  # List of up to 3 timestamps\n",
    "        else:\n",
    "            print(f\"Failed for {title}: {response.status_code}\")\n",
    "            return [None, None, None]\n",
    "    except Exception as e:\n",
    "        print(f\"Error for {title}: {e}\")\n",
    "        return [None, None, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d94eca4c-a298-4dd0-91e8-90b16690a196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Collecting edit counts...\n",
      "⏳ Collecting last 3 edit timestamps...\n",
      "⏳ Collecting number of unique editors...\n",
      "✅ All features saved to C:\\Users\\nafis\\Downloads\\pageid_title_sample.csv\n"
     ]
    }
   ],
   "source": [
    "# 🏃‍♀️ Apply all functions to first 100 articles\n",
    "print(\"⏳ Collecting edit counts...\")\n",
    "titles_sample['edit_count'] = titles_sample['title'].apply(get_edit_count)\n",
    "\n",
    "print(\"⏳ Collecting last 3 edit timestamps...\")\n",
    "titles_sample[['edit_1', 'edit_2', 'edit_3']] = pd.DataFrame(\n",
    "    titles_sample['title'].apply(get_last_3_edits).tolist(),\n",
    "    index=titles_sample.index\n",
    ")\n",
    "\n",
    "print(\"⏳ Collecting number of unique editors...\")\n",
    "titles_sample['num_editors'] = titles_sample['title'].apply(get_num_editors)\n",
    "\n",
    "# ✅ Save all results at once\n",
    "output_path = r\"C:\\Users\\nafis\\Downloads\\pageid_title_sample.csv\"\n",
    "titles_sample.to_csv(output_path, index=False)\n",
    "print(f\"✅ All features saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a768048-ac57-43b8-a391-ea222cb4f539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
