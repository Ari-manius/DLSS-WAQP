---
title: "Wikipedia Article Quality Prediction"
subtitle: "Final Report: DLSS"
author: 
  - name: "Nafiß Tavakol, Kuon Ito, Lorenz, Rückert, Marius Helten"
    affiliation: "1406810"
editor: visual
date: "`r format(Sys.Date(), '%B %d, %Y')`" 
format:
  pdf:
    fig-width: 6
    fig-height: 3
    keep-tex: false  
    colorlinks: true
    documentclass: report
    links-as-notes: true
    include-in-header: 
      text: |
        \usepackage{float}
header-includes:
  - \usepackage{titling}  
  - \pretitle{\begin{center}\LARGE\bfseries} 
  - \posttitle{\end{center}}  
  - \preauthor{\begin{center} \large} 
  - \postauthor{\end{center}} 
  - \predate{\begin{center}\large} 
  - \postdate{\begin{figure}[H]
      \centering
      \includegraphics[width=1.0\textwidth]{Images/placeholder.png}
    \end{figure}
    \end{center}} 
bibliography: references.bib
cite-method: citeproc
link-citations: true
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(error = TRUE)
```

```{r, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
data_descriptive <- read.csv("/Users/ramius/Desktop/CodeVault/01_Project/Uni/DLSS_DeepLearningforSocialScientists/Final_Project/DLSS-WAQP/2_FinalReport/Tables/Wiki_metrics.csv")

data_homophily <- read.csv("/Users/ramius/Desktop/CodeVault/01_Project/Uni/DLSS_DeepLearningforSocialScientists/Final_Project/DLSS-WAQP/2_FinalReport/Tables/Wiki_assortativity.csv")
```

# Introduction

(Problem description and approach)

Citing this way: [-@hanGraphNeuralNetworks2020], [-@montiFakeNewsDetection2019]

# Data Analysis

## Dataset Description

(Features and Target Variable, Article based)

## Wikipedia Network

The network was obtained by a BFS-search, starting at a handful of seed articles.

### Graph Description

-   single CC
-   directed network
-   sparse network
-   reasonably clustered
-   pretty sizeable sample but by no means exhaustive for all of wikipedia

```{r, echo=FALSE}
knitr::kable(data_descriptive, caption = "Network Descriptive Metrics", 
              digits = 5, 
              format.args = list(scientific = FALSE))
```

-   Degree Distribution
-   reasonably similar to powerlaw or lognormal - in any case heavy tailed (very few nodes with a lot of connections), pretty normal for many internet networks, rich-get richer effect

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=5cm, keepaspectratio]{Images/Degree_PowerLaw.png}
    \caption{Wikipedia Graph - Degree Distribution and Power Law Analysis}
    \label{fig:pca_combined}
\end{figure}

-   for none of the tested attributes assortativity could be measured, which is very surprising and also pretty disheartneing
-   basically similar nodes are not neighbors which takes away one of the main theoretic assumptions

```{r, echo=FALSE}
knitr::kable(data_homophily, caption = "Network Assortativity", 
              digits = 4, 
              format.args = list(scientific = FALSE))
```

-   Transition Probabilties for all classes on graph

### Graph based Features

-   For one we used the conenctions in the network as part of the data that all the neural networks except for the MLP-Baseline were trained

-   Centralities

    -   pagerank
    -   katz (paused beacause of redundancy)
    -   betweenness
    -   hub/authority
    -   degree in
    -   degree out
    -   core number
    -   clustering (local)

-   CCDFs of network metrics across article categories

-   Spectral Embedding

-   Transition Matrix

-   Modularity Matrix

-   Share Reciprocity

## Preprocessing

During the pre-processing we face two important challenges. The first was the highly imbalanced target variable with only a very small fraction of high-quality articles. The second challenge were the heavily skewed distributions, especially for the graph based features.

Regarding our target variable we tried switching from a classification to a regression problem. This allowed us to circumvent class counts by treating the ordinal categorical attributes as a numerical attribute. The heavily skewed distribution was log-transformed in order to obtain a less skewed distribution. After a brief evaluation, this approach proved to be flawed because the model just made average predictions. We concentrated on the classification approach. A first measure was to aggregated the categories into three ordinal classes from the initial count of nine. This improved class frequencies to a reasonable degree where training and predictions became possible.

The features had to be preprocessed since the numerical ranges were not uniform and fit for training. Here normalization could have solved the problem. However the distributions particularly for the network metrics are immensely positvely skewed. Different kinds such as standard, minmax, robust and robust-log scaling proved to be ineffective to generate reasonably spread distributions. The only approach that brought reasonable results was quantile scaling. Quantile scaling transforms data by mapping each value to its percentile rank, creating a uniform distribution where extreme outliers get compressed while preserving relative order.

For the network we remove nodes with total degree(k)$k ≤ 1$, so the leaves of the network. \# Methods

## Graph-Neural-Network Models

### Graph-Convolutional

### Graph-Sage

### Graph Attention

## Training and Evaluation

# Results

(Performance comparison tables, learning curves, confusion matrices)

# Discussion and Conclusion

(Performance Comparison, Interpret the results in the context of social network theory, Key Findings and Implications)

# References

## Code and Data

-   API´s
-   Python Packages

## Literature

-   Citeable papers