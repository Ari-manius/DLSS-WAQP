---
title: "Wikipedia Article Quality Prediction"
subtitle: "Final Report: DLSS"
author: 
  - name: "Nafiß Tavakol, Kuon Ito, Lorenz, Rückert, Marius Helten"
    affiliation: "1406810"
editor: visual
date: "`r format(Sys.Date(), '%B %d, %Y')`" 
format:
  pdf:
    fig-width: 6
    fig-height: 3
    keep-tex: false  
    colorlinks: true
    documentclass: report
    links-as-notes: true
    include-in-header: 
      text: |
        \usepackage{float}
header-includes:
  - \usepackage{titling}  
  - \pretitle{\begin{center}\LARGE\bfseries} 
  - \posttitle{\end{center}}  
  - \preauthor{\begin{center} \large} 
  - \postauthor{\end{center}} 
  - \predate{\begin{center}\large} 
  - \postdate{\begin{figure}[H]
      \centering
      \includegraphics[width=1.0\textwidth]{Images/placeholder.png}
    \end{figure}
    \end{center}} 
bibliography: references.bib
cite-method: citeproc
link-citations: true
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(error = TRUE)
```

```{r, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
data_descriptive <- read.csv("/Users/ramius/Desktop/CodeVault/01_Project/Uni/DLSS_DeepLearningforSocialScientists/Final_Project/DLSS-WAQP/2_FinalReport/Tables/Wiki_metrics.csv")

data_homophily <- read.csv("/Users/ramius/Desktop/CodeVault/01_Project/Uni/DLSS_DeepLearningforSocialScientists/Final_Project/DLSS-WAQP/2_FinalReport/Tables/Wiki_assortativity.csv")
```

# Introduction

Citing this way: [-@hanGraphNeuralNetworks2020], [-@montiFakeNewsDetection2019]

Wikipedia has become one of the most prominent sources of freely accessible knowledge worldwide, attracting millions of readers across domains such as history, politics, science, and popular culture [-@hortaRibeiro2020]. Its openness to user contributions is both a strength—encouraging collaborative knowledge construction—and a weakness, since it may also lead to the creation of low-quality or biased articles [-@bassaniQualityWikipedia2020]. While the platform employs a quality grading scheme ranging from Featured Articles (FA) to Stub-class entries to guide improvement efforts [-@warnckeWang2015; -@bassaniQualityWikipedia2020], only a very small fraction of articles meet the highest standards, with just 0.09% classified as FA and 0.5% as Good Articles [-@warnckeWang2015].

To cope with this challenge, both manual and automated quality assessment approaches have been explored. Human volunteers and WikiProjects monitor articles, but the scale of Wikipedia and the subjectivity of evaluations make this process inconsistent and insufficient [-@bassaniQualityWikipedia2020]. Early automated methods relied on handcrafted features of article text [-@halfakerGeiger2019], while subsequent research applied machine learning techniques such as doc2vec [-@leMikolov2014], BiLSTMs [-@shen2017], and multimodal approaches incorporating images and metadata [-@shen2019]. The Neural Wikipedia Quality Monitor (NwQM) further demonstrated that integrating multiple modalities—including article text, metadata, and talk pages—substantially improves prediction performance [-@reddyNwQM2020]. Similarly, supervised classification frameworks have been developed using syntactic, stylistic, and editorial-history features, combined with labeled ground-truth datasets, to classify articles across the Wikipedia quality scale [-@bassaniQualityWikipedia2020].

Another crucial dimension of quality in Wikipedia is verifiability. The Verifiability policy requires that claims be supported by reliable sources, and unsourced material may be challenged with a {citation needed} tag [-@rediCitationNeeded2019]. Despite this, many articles contain unverified claims, and citation practices among editors are often ad hoc rather than systematic. As of 2019, over 350,000 articles contained at least one {citation needed} flag, but the actual number of unverified statements is likely far higher [-@rediCitationNeeded2019]. Understanding citation practices is therefore central to ensuring Wikipedia’s reliability, since citations provide a strong signal of article quality and credibility.

Recent advances in graph representation learning have created new opportunities to model Wikipedia not only through its textual and editorial features, but also through its citation structures. Graph Neural Networks (GNNs) have proven highly effective at learning from graph-structured data, with applications in semi-supervised node classification and link prediction [-@kipfWelling2016a; -@velickovic2017; -@wu2019]. Traditionally, benchmarks for GNNs have relied on citation networks such as Cora, CiteSeer, and PubMed [-@yang2016]. However, these datasets share similar structural properties and inconsistent training splits, limiting fair evaluation [-@shchur2018; -@klicpera2018]. To address this, the Wiki-CS dataset was introduced as a Wikipedia-based benchmark with higher connectivity and structural diversity, offering a richer environment for testing GNN performance [-@mernyeiWikiCS2020].

Building on these insights, this project explores the potential of Wikipedia’s citation networks as a foundation for predicting article quality ratings. By leveraging Graph Neural Networks, we aim to capture how signals of reliability and authority flow through citation structures. This network-oriented perspective complements prior text-based and multimodal approaches, addressing gaps in the literature and offering new insights into the relationship between verifiability, article interconnectedness, and collaborative knowledge quality.

Related work

Bassani and Viviani [-@bassaniQualityWikipediaArticles2019] proposed a supervised classification approach for Wikipedia article quality, introducing 264 handcrafted features spanning text, review history, and network dimensions. They highlighted the importance of reliable ground truth construction, showing that inconsistencies between labeled and current article versions can reduce accuracy. Their experiments demonstrated that text features were the most effective, while network features contributed less, and that Gradient Boosting achieved the best performance, reaching 62% accuracy in multi-class classification.

Reddy et al. [-@reddyNwQMNeuralQuality2020] introduced NwQM, a neural framework for article quality assessment that integrates text, metadata, and images to build multimodal representations. Unlike earlier approaches relying primarily on structural or handcrafted features [-@halfakerResearchShowcasesObjective2015; -@dangMeasuringQualityWikipedia2016], their model demonstrated an 8% improvement over state-of-the-art methods, establishing the value of multimodal learning in quality prediction.

Redi et al. [-@rediCitationNeededTaxonomy2019] examined verifiability in Wikipedia by introducing a taxonomy of citation reasons and developing models to predict both citation need and citation purpose. Their findings emphasized that citations are critical for maintaining Wikipedia’s reliability, with certain types of content—such as historical facts, statistics, or reported speech—being particularly likely to require verification.

Mernyei and Cangea [-@mernyeiWikiCSWikipediaBasedBenchmark2020] proposed Wiki-CS, a benchmark dataset for evaluating Graph Neural Networks (GNNs). Unlike traditional citation network datasets such as Cora, CiteSeer, or PubMed, Wiki-CS offers higher connectivity and a richer structure, making it a more challenging and diverse testbed. It is primarily used for semi-supervised node classification and provides standardized splits, addressing reproducibility issues in GNN research.

Together, these studies highlight the progression from handcrafted features to multimodal learning and benchmark datasets, and they underscore the importance of verifiability and structured evaluation. Building on these insights, our project focuses on leveraging citation networks and GNNs to capture how quality signals propagate across interconnected articles, thereby offering a complementary network-based perspective on Wikipedia article quality assessment.

# Data Analysis

## Dataset Description

(Features and Target Variable, Article based)

## Wikipedia Network

The network was obtained by a BFS-search, starting at a handful of seed articles.

### Graph Description

-   single CC
-   directed network
-   sparse network
-   reasonably clustered
-   pretty sizeable sample but by no means exhaustive for all of wikipedia

```{r, echo=FALSE}
knitr::kable(data_descriptive, caption = "Network Descriptive Metrics", 
              digits = 5, 
              format.args = list(scientific = FALSE))
```

-   Degree Distribution
-   reasonably similar to powerlaw or lognormal - in any case heavy tailed (very few nodes with a lot of connections), pretty normal for many internet networks, rich-get richer effect

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=5cm, keepaspectratio]{Images/Degree_PowerLaw.png}
    \caption{Wikipedia Graph - Degree Distribution and Power Law Analysis}
    \label{fig:pca_combined}
\end{figure}

-   for none of the tested attributes assortativity could be measured, which is very surprising and also pretty disheartneing
-   basically similar nodes are not neighbors which takes away one of the main theoretic assumptions

```{r, echo=FALSE}
knitr::kable(data_homophily, caption = "Network Assortativity", 
              digits = 4, 
              format.args = list(scientific = FALSE))
```

-   Transition Probabilties for all classes on graph

### Graph based Features

-   For one we used the conenctions in the network as part of the data that all the neural networks except for the MLP-Baseline were trained

-   Centralities

    -   pagerank
    -   katz (paused beacause of redundancy)
    -   betweenness
    -   hub/authority
    -   degree in
    -   degree out
    -   core number
    -   clustering (local)

-   CCDFs of network metrics across article categories

-   Spectral Embedding

-   Transition Matrix

-   Modularity Matrix

-   Share Reciprocity

## Preprocessing

During the pre-processing we face two important challenges. The first was the highly imbalanced target variable with only a very small fraction of high-quality articles. The second challenge were the heavily skewed distributions, especially for the graph based features.

Regarding our target variable we tried switching from a classification to a regression problem. This allowed us to circumvent class counts by treating the ordinal categorical attributes as a numerical attribute. The heavily skewed distribution was log-transformed in order to obtain a less skewed distribution. After a brief evaluation, this approach proved to be flawed because the model just made average predictions. We concentrated on the classification approach. A first measure was to aggregated the categories into three ordinal classes from the initial count of nine. This improved class frequencies to a reasonable degree where training and predictions became possible.

The features had to be preprocessed since the numerical ranges were not uniform and fit for training. Here normalization could have solved the problem. However the distributions particularly for the network metrics are immensely positvely skewed. Different kinds such as standard, minmax, robust and robust-log scaling proved to be ineffective to generate reasonably spread distributions. The only approach that brought reasonable results was quantile scaling. Quantile scaling transforms data by mapping each value to its percentile rank, creating a uniform distribution where extreme outliers get compressed while preserving relative order.

For the network we remove nodes with total degree(k)$k ≤ 1$, so the leaves of the network. \# Methods

## Graph-Neural-Network Models

### Graph-Convolutional

### Graph-Sage

### Graph Attention

## Training and Evaluation

# Results

(Performance comparison tables, learning curves, confusion matrices)

# Discussion and Conclusion

(Performance Comparison, Interpret the results in the context of social network theory, Key Findings and Implications)

# References

## Code and Data

-   API´s
-   Python Packages

## Literature

-   Citeable papers
