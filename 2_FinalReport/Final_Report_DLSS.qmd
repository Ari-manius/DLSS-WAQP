---
title: "Wikipedia Article Quality Prediction"
subtitle: "Final Report: DLSS"
author: 
  - name: "Nafiseh Tavakol, Kuon Ito, Lorenz, Rückert, Marius Helten"
    affiliation: "1406810"
editor: visual
date: "`r format(Sys.Date(), '%B %d, %Y')`" 
format:
  pdf:
    colorlinks: true
    links-as-notes: true
    include-in-header: |
      % Margins
      \geometry{top=0.7in, bottom=0.7in, left=0.6in, right=0.6in}

      % Slightly tighter line spacing (helps overall)
      \usepackage{setspace}
      \setstretch{1.02}

      % Force compact paragraphs after all packages load
      \AtBeginDocument{%
        \setlength{\parskip}{0pt}%  no vertical gap between paragraphs
        \setlength{\parindent}{1em}% small indent to mark new paragraph
      }

      % Most visible spacing is around headings – tighten it
      \usepackage{titlesec}
      \titlespacing*{\section}{0pt}{0.6\baselineskip}{0.25\baselineskip}
      \titlespacing*{\subsection}{0pt}{0.45\baselineskip}{0.2\baselineskip}
      \titlespacing*{\subsubsection}{0pt}{0.4\baselineskip}{0.18\baselineskip}

      % Lists also add space – remove it
      \usepackage{enumitem}
      \setlist{nosep}
    

header-includes:
  - \usepackage{titling}  
  - \pretitle{\begin{center}\LARGE\bfseries} 
  - \posttitle{\end{center}}  
  - \preauthor{\begin{center} \large} 
  - \postauthor{\end{center}} 
  - \predate{\begin{center}\large} 
  - \postdate{\begin{figure}[H]
      \centering
      \includegraphics[width=1.0\textwidth]{Images/placeholder.png}
    \end{figure}
    \end{center}} 
bibliography: references.bib
cite-method: citeproc
link-citations: true
execute:
  echo: false
  warning: false
  message: false
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(error = TRUE)
```

```{r, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
data_descriptive <- read.csv("/Users/ramius/Desktop/CodeVault/01_Project/Uni/DLSS_DeepLearningforSocialScientists/Final_Project/DLSS-WAQP/2_FinalReport/Tables/Wiki_metrics.csv")

data_homophily <- read.csv("/Users/ramius/Desktop/CodeVault/01_Project/Uni/DLSS_DeepLearningforSocialScientists/Final_Project/DLSS-WAQP/2_FinalReport/Tables/Wiki_assortativity.csv")
```

# **Introduction**

The Web enables anyone to read, publish, and share information at unprecedented speed and scale, greatly benefiting billions but also creating fertile ground for falsehoods [@kumar2016disinformation]. Wikipedia, as one of the most widely used sources of free knowledge, faces credibility concerns due to hoaxes and the risk of low-quality or biased contributions [@hortaribeiro2020sudden; @kumar2016disinformation; @bassani2019quality]. Although the platform employs a grading scheme from Featured Articles (FA) to Stubs, only a very small fraction of articles reach the highest quality levels, creating an imbalance that resembles anomaly detection, where rare but important cases must be identified [@warnckewang2015success; @bassani2019quality]. To address this, both manual and automated quality assessment methods have been explored. Human volunteers and WikiProjects monitor content, but scale and subjectivity limit their effectiveness. Automated approaches progressed from handcrafted textual features to machine learning models such as doc2vec [@le2014distributed], BiLSTMs, and multimodal systems that integrate images and metadata. Reddy et al. [@reddy2021nwqm] showed that multimodal learning substantially improves prediction, while Bassani and Viviani [@bassani2019quality] highlighted the challenges of reliable ground truth and found textual features more predictive than network ones. Verifiability is another key dimension: Redi et al. [@redi2019citation] introduced a taxonomy of citation reasons and showed that citation practices strongly signal credibility. Yet, as of 2019, more than 350,000 articles carried a tag, suggesting widespread unverified claims. Recent advances in Graph Neural Networks (GNNs) open new opportunities to model Wikipedia not only through text but also through its citation structures. Traditional citation benchmarks (Cora, CiteSeer, PubMed) suffer from limited diversity [@yang2016revisiting; @shchur2018pitfalls], leading to the introduction of Wiki-CS, a richer Wikipedia-based dataset [@mernyei2020wikics]. Within this landscape, approaches can be divided into content-based, focusing on semantics and syntax, and context-based, emphasizing external signals such as social or citation networks [@montiFakeNewsDetection2019].

Our project adopts a context-based perspective, leveraging both article relations (external references) and article structure (sections, citations, length). By applying GNNs, we aim to model how signals of reliability and authority propagate through these networks, while complementing them with additional structural features. This network-oriented approach avoids reliance on semantics or style, offering a generalizable and scalable framework for predicting Wikipedia article quality.

## Data Analysis

### Dataset Description

### Features:

A dataset of 379,926 English Wikipedia articles was assembled to support the quality prediction task. The collection combines article-level features, structural metadata, and editing history, enabling both content-independent and behavioral dimensions of quality to be examined. Articles span the full range of Wikipedia’s grading scheme, from Stub to Featured Article (FA), ensuring coverage of different writing styles, completeness levels, and editorial efforts. The design of this dataset is informed by prior research on text, structure, and verifiability [-@bassani2019quality], [-@reddy2021nwqm], [-@redi2019citation] as well as graph-based benchmarks such as Wiki-CS [-@mernyei2020wikics]. Drawing on these insights, the dataset integrates both article-level descriptors and network-oriented variables.

For each article, descriptive attributes include page length, number of references, number of sections, templates, infobox presence, and pageviews. Structural metadata records the number of categories, links, and depth in the category hierarchy. Editorial activity is tracked through detailed revision histories, separating human and bot edits and further distinguishing between registered, anonymous, and automated accounts. To reflect recent collaboration dynamics, edit-related variables were restricted to the past two years. Finally, additional context such as last edit timestamp, days since last edit, and protection status was included to capture recency and stability. This design results in a dataset that captures both structural and editorial signals, complementing traditional content-based features and enabling a multi-perspective analysis of Wikipedia article quality.

```{r}
library(knitr)
library(dplyr)

# Define variables, categories, and definitions
vars <- tribble(
  ~Category, ~Variable, ~Definition,
  "Structure","num_categories","Number of categories assigned to the article.",
  "Structure","num_links","Total number of internal/external links.",
  "Structure","page_length","Length of the article (characters).",
  "Structure","num_references","Number of citations in the article.",
  "Structure","num_sections","Number of sections.",
  "Structure","num_templates","Number of templates used.",
  "Structure","has_infobox_encoded","1 if an infobox exists, otherwise 0.",
  "Structure","protection_status_encoded","Encoded protection level.",
  "Style / Semantic","assessment_source_umap_1","UMAP dim 1 of assessment source.",
  "Style / Semantic","assessment_source_umap_2","UMAP dim 2 of assessment source.",
  "Style / Semantic","assessment_source_umap_3","UMAP dim 3 of assessment source.",
  "Network","days_since_last_edit","Days since the last edit.",
  "Network","edits_all_types","Total edits (last two years).",
  "Network","edits_anonymous","Anonymous edits (last two years).",
  "Network","edits_bot","Bot edits (last two years).",
  "Network","edits_group_bot","Group-bot edits (last two years).",
  "Network","edits_human","Human edits (last two years).",
  "Network","edits_name_bot","Named-bot edits (last two years).",
  "Network","edits_user","Registered-user edits (last two years).",
  "Network","pageviews_Jul2023Jul2024","Pageviews from Jul 2023–Jul 2024."
)

# Render table
kable(vars, caption = "Variables grouped by category with their definitions")
```

### Edges: ......

### Target Variable:

Wikipedia articles are rated on an ordinal quality scale. In this project the following classes are used as the target: FA, FL, FM, A, GA, B, C, Start, Stub, List.

```{r}
library(knitr)
library(dplyr)

quality_classes <- tribble(
  ~Class, ~Meaning,
  "FA", "Featured Article – highest quality, comprehensive and well-sourced",
  "FL", "Featured List – best-quality lists, complete and well-referenced",
  "FM", "Featured Media – high-quality non-textual media (images, videos, etc.)",
  "A", "Near-featured quality, but may need minor improvements",
  "GA", "Good Article – accurate, well-structured, but less comprehensive than FA",
  "B", "Mostly complete, but still lacking references or polish",
  "C", "Useful coverage, but incomplete or missing important details",
  "Start", "Basic coverage, underdeveloped but beyond stub level",
  "Stub", "Very short or incomplete article, minimal information",
  "List", "Articles in list format, assessed on completeness and structure"
)

kable(quality_classes, caption = "Wikipedia quality assessment classes and their meaning")
```

### Data Collection:

This project combined four complementary data sources to capture different aspects of Wikipedia articles: the Wikipedia Dump (raw text and structure), the Pageviews API (popularity and user attention), the Edit History API (editorial activity patterns, including user and bot edits), and the Wikipedia API (article crawling and network construction). Article metadata was retrieved by mapping page IDs to titles via the MediaWiki API, which only supports up to 50 IDs per request; page IDs were split into batches of 50, queried in parallel with ThreadPoolExecutor, and merged back into the dataset before saving to CSV. One year of pageview data (July 2023–July 2024) was collected for each article from the Wikimedia REST API, aggregated into annual totals, and stored incrementally to prevent data loss; parallel requests and tqdm progress tracking ensured efficiency. Temporal metadata was added by retrieving last edit timestamps through the REST API’s `/page/summary/{title}` endpoint, using randomized delays (0.3–0.6s), retry logic for HTTP 429 errors, and parallel workers to accelerate processing; results were saved to `final_last_edit.csv`. Editorial activity was captured for July 2023–July 2025, with edit counts broken down by registered users, anonymous users, group bots, and name bots, then aggregated into human vs. bot contributions. To respect API limits, randomized delays, proxy rotation, and periodic checkpoints were used, and data collection was parallelized for efficiency. Together, these steps produced a comprehensive dataset covering article text and structure, popularity, recency, editorial activity, and network relationships, providing a robust foundation for downstream analyses.

### Article Target Feature Processing

The dataset was prepared for modeling by constructing target labels and encoding structured features. Using Polars, articles were indexed by title, page ID, and numeric identifiers for efficient lookup. Each article was mapped to its Wikipedia quality class (FA, GA, B, etc.), from which three target variables were derived: a 10-level ordinal scale (`Target_QC_cat`), a 3-tier aggregate scale (`Target_QC_aggcat`), and a log-transformed numeric variant (`Target_QC_numlog`). Categorical and binary attributes were encoded, including protection status (integer labels), infobox presence (binary), and assessment source (one-hot, then reduced with UMAP). The final feature set integrated content metrics (page length, sections, templates, references, categories, links), editorial activity (days since last edit, human vs. bot edits), and popularity (annual pageviews, July 2023–July 2024). Together, these features capture structural, editorial, and popularity dimensions of Wikipedia articles, providing a comprehensive representation for graph construction and machine learning models.

### Graph Preparation for GNN Training

The Wikipedia dataset was converted from graph-tool format into PyTorch Geometric Data objects, with node features, edge features, and target labels systematically encoded (numeric, boolean, and categorical via label or one-hot encoding). To stabilize model training, node features were standardized using multiple scaling techniques (StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer, PowerTransformer, and log-based scaling), producing several dataset variants. Target variables were derived from `Target_` attributes, with `Target_QC_aggcat` used as the primary classification label. Each processed graph was stored in two forms: a PyTorch tensor dataset (`.pt`) for GNN training and a Parquet file for feature inspection and debugging. This pipeline yielded clean, scalable graph data suitable for downstream learning on Wikipedia article quality prediction.

### Data Exploration

The dataset comprises \~380,000 Wikipedia articles labeled with quality classes, but the distribution is highly imbalanced: most articles fall into low-quality categories (Stub, Start), while only a small minority reach high-quality levels (FA, GA, FL, A). Quality progression is evident—higher-quality articles are much longer and include richer structural and citation features such as references, links, and sections. In contrast, Stub and Start articles remain short and sparsely referenced, reflecting limited editorial development. These patterns confirm that structural richness and citation density are closely associated with editorial quality.

```{r}
library(tibble)
library(dplyr)
library(scales)
library(knitr)

tbl <- tribble(
  ~`Quality Class`, ~Count, ~`Avg. Page Length`, ~`Avg. References`, ~`Avg. Links`, ~`Avg. Sections`,
  "A",     113,    60096.51,  97.15, 386.11, 17.62,
  "B",   29768,    61135.93,  96.63, 422.76, 21.02,
  "C",   74983,    33138.79,  47.86, 285.05, 14.69,
  "FA",   1582,    89048.49, 142.50, 514.55, 21.72,
  "FL",    320,    58989.33,  87.65, 370.57, 11.29,
  "GA",   5934,    64000.73, 115.96, 395.99, 17.88,
  "List", 13161,   29490.59,  29.69, 368.69, 13.75,
  "Start",162145,  14064.61,  17.70, 179.28,  8.25,
  "Stub", 91920,    5878.57,   6.29, 143.61,  4.21
) |>
  mutate(
    Count = comma(Count),
    `Avg. Page Length` = comma(`Avg. Page Length`, accuracy = 0.01),
    `Avg. References`  = number(`Avg. References`,  accuracy = 0.01),
    `Avg. Links`       = number(`Avg. Links`,       accuracy = 0.01),
    `Avg. Sections`    = number(`Avg. Sections`,    accuracy = 0.01)
  )

kable(tbl, caption = "Quality classes and average structural metrics.")
```

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=5cm, keepaspectratio]{Images/quality_distribution.png}
    \caption{Wikipedia Graph - Degree Distribution and Power Law Analysis}
    \label{fig:pca_combined}
\end{figure}

The heatmap shows strong correlations among structural features, with the highest between page length and references (0.86), indicating that longer articles are usually better structured and more thoroughly referenced. Links are also positively correlated but provide partly independent information. A log–log scatter plot of links versus references confirms this trend: articles with more links often include more references, though variation remains, showing that links and references capture complementary aspects of article richness.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=5cm, keepaspectratio]{Images/features_corr.png}
    \caption{Wikipedia Graph - Degree Distribution and Power Law Analysis}
    \label{fig:pca_combined}
\end{figure}

In Feature Distributions plots, most articles cluster at the low end for page length, references, links, sections, pageviews, and recency of edits, with only a few outliers reaching extreme values—reflecting Wikipedia’s heterogeneity, where a small subset dominates in depth and attention.. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=5cm, keepaspectratio]{Images/features_dis.png}
    \caption{Wikipedia Graph - Degree Distribution and Power Law Analysis}
    \label{fig:pca_combined}
\end{figure}

Pageviews vary widely across classes. While Featured Articles (FA) and Good Articles (GA) generally attract higher median views, many B-class and even lower-quality articles also reach high visibility. This suggests that popularity is not fully aligned with editorial quality, articles can be widely read even if their structural quality is limited.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=5cm, keepaspectratio]{Images/pageview.png}
    \caption{Wikipedia Graph - Degree Distribution and Power Law Analysis}
    \label{fig:pca_combined}
\end{figure}

Feature Relationships Pairwise feature comparisons show clear clustering by quality: high-quality articles combine length, references, links, and sections in consistent proportions, while low-quality articles remain compact across all dimensions. Pageviews and recency of edits add further variation but only partially align with quality, reinforcing that structural completeness and editorial effort are the strongest signals of quality.

<!-- ![](images/clipboard-2781340232.png){width="469" height="322"}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=5cm, keepaspectratio]{Images/clipboard-2781340232.png}
    \caption{Wikipedia Graph - Degree Distribution and Power Law Analysis}
    \label{fig:pca_combined}
\end{figure} -->

## Wikipedia Network

The network was obtained by a BFS-search, starting at a handful of seed articles.

### Graph Description

-   single CC
-   directed network
-   sparse network
-   reasonably clustered
-   pretty sizeable sample but by no means exhaustive for all of wikipedia

```{r, echo=FALSE}
knitr::kable(data_descriptive, caption = "Network Descriptive Metrics", 
              digits = 5, 
              format.args = list(scientific = FALSE))
```

-   Degree Distribution
-   reasonably similar to powerlaw or lognormal - in any case heavy tailed (very few nodes with a lot of connections), pretty normal for many internet networks, rich-get richer effect

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=5cm, keepaspectratio]{Images/Degree_PowerLaw.png}
    \caption{Wikipedia Graph - Degree Distribution and Power Law Analysis}
    \label{fig:pca_combined}
\end{figure}

-   for none of the tested attributes assortativity could be measured, which is very surprising and also pretty disheartneing
-   basically similar nodes are not neighbors which takes away one of the main theoretic assumptions

```{r, echo=FALSE}
knitr::kable(data_homophily, caption = "Network Assortativity", 
              digits = 4, 
              format.args = list(scientific = FALSE))
```

-   Transition Probabilties for all classes on graph

### Graph based Features

-   For one we used the conenctions in the network as part of the data that all the neural networks except for the MLP-Baseline were trained

-   Centralities

    -   pagerank
    -   katz (paused beacause of redundancy)
    -   betweenness
    -   hub/authority
    -   degree in
    -   degree out
    -   core number
    -   clustering (local)

-   CCDFs of network metrics across article categories

-   Spectral Embedding

-   Transition Matrix

-   Modularity Matrix

-   Share Reciprocity

## Preprocessing

During the pre-processing we face two important challenges. The first was the highly imbalanced target variable with only a very small fraction of high-quality articles. The second challenge were the heavily skewed distributions, especially for the graph based features.

Regarding our target variable we tried switching from a classification to a regression problem. This allowed us to circumvent class counts by treating the ordinal categorical attributes as a numerical attribute. The heavily skewed distribution was log-transformed in order to obtain a less skewed distribution. After a brief evaluation, this approach proved to be flawed because the model just made average predictions. We concentrated on the classification approach. A first measure was to aggregated the categories into three ordinal classes from the initial count of nine. This improved class frequencies to a reasonable degree where training and predictions became possible.

The features had to be preprocessed since the numerical ranges were not uniform and fit for training. Here normalization could have solved the problem. However the distributions particularly for the network metrics are immensely positvely skewed. Different kinds such as standard, minmax, robust and robust-log scaling proved to be ineffective to generate reasonably spread distributions. The only approach that brought reasonable results was quantile scaling. Quantile scaling transforms data by mapping each value to its percentile rank, creating a uniform distribution where extreme outliers get compressed while preserving relative order.

For the network we remove nodes with total degree(k)$k ≤ 1$, so the leaves of the network. \# Methods

## Graph-Neural-Network Models

### Graph-Convolutional

### Graph-Sage

### Graph Attention

## Training and Evaluation

# Results

(Performance comparison tables, learning curves, confusion matrices)

# Discussion and Conclusion

(Performance Comparison, Interpret the results in the context of social network theory, Key Findings and Implications)

# References

## Code and Data

-   API´s
-   Python Packages

## Literature

-   Citeable papers