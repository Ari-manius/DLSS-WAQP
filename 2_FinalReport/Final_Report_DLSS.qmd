---
title: "Wikipedia Article Quality Prediction"
subtitle: "Final Report: DLSS"
author: 
  - name: "Nafiseh Tavakol, Kuon Ito, Lorenz, Rückert, Marius Helten"
    affiliation: "1406810"
editor: visual
date: "`r format(Sys.Date(), '%B %d, %Y')`" 
format:
  pdf:
    colorlinks: true
    links-as-notes: true
    include-in-header: |
      % Margins
      \geometry{top=0.7in, bottom=0.7in, left=0.6in, right=0.6in}

      % Slightly tighter line spacing (helps overall)
      \usepackage{setspace}
      \setstretch{1.02}

      % Force compact paragraphs after all packages load
      \AtBeginDocument{%
        \setlength{\parskip}{0pt}%  no vertical gap between paragraphs
        \setlength{\parindent}{1em}% small indent to mark new paragraph
      }

      % Most visible spacing is around headings – tighten it
      \usepackage{titlesec}
      \titlespacing*{\section}{0pt}{0.6\baselineskip}{0.25\baselineskip}
      \titlespacing*{\subsection}{0pt}{0.45\baselineskip}{0.2\baselineskip}
      \titlespacing*{\subsubsection}{0pt}{0.4\baselineskip}{0.18\baselineskip}

      % Lists also add space – remove it
      \usepackage{enumitem}
      \setlist{nosep}
    

header-includes:
  - \usepackage{titling}  
  - \pretitle{\begin{center}\LARGE\bfseries} 
  - \posttitle{\end{center}}  
  - \preauthor{\begin{center} \large} 
  - \postauthor{\end{center}} 
  - \predate{\begin{center}\large} 
  - \postdate{\begin{figure}[H]
      \centering
      \includegraphics[width=1.0\textwidth]{Images/placeholder.png}
    \end{figure}
    \end{center}} 
bibliography: references.bib
cite-method: citeproc
link-citations: true
execute:
  echo: false
  warning: false
  message: false
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(error = TRUE)
```

```{r, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
data_descriptive <- read.csv("/Users/ramius/Desktop/CodeVault/01_Project/Uni/DLSS_DeepLearningforSocialScientists/Final_Project/DLSS-WAQP/2_FinalReport/Tables/Wiki_metrics.csv")

data_homophily <- read.csv("/Users/ramius/Desktop/CodeVault/01_Project/Uni/DLSS_DeepLearningforSocialScientists/Final_Project/DLSS-WAQP/2_FinalReport/Tables/Wiki_assortativity.csv")
```

# **Introduction**

The Web enables anyone to read, publish, and share information at unprecedented speed and scale, greatly benefiting billions but also creating fertile ground for falsehoods [@kumar2016disinformation]. Wikipedia, as one of the most widely used sources of free knowledge, faces credibility concerns due to hoaxes and the risk of low-quality or biased contributions [@hortaribeiro2020sudden; @kumar2016disinformation; @bassani2019quality]. Although the platform employs a grading scheme from Featured Articles (FA) to Stubs, only a very small fraction of articles reach the highest quality levels, creating an imbalance that resembles anomaly detection, where rare but important cases must be identified [@warnckewang2015success; @bassani2019quality]. To address this, both manual and automated quality assessment methods have been explored. Human volunteers and WikiProjects monitor content, but scale and subjectivity limit their effectiveness. Automated approaches progressed from handcrafted textual features to machine learning models such as doc2vec [@le2014distributed], BiLSTMs, and multimodal systems that integrate images and metadata. Reddy et al. [@reddy2021nwqm] showed that multimodal learning substantially improves prediction, while Bassani and Viviani [@bassani2019quality] highlighted the challenges of reliable ground truth and found textual features more predictive than network ones. Verifiability is another key dimension: Redi et al. [@redi2019citation] introduced a taxonomy of citation reasons and showed that citation practices strongly signal credibility. Yet, as of 2019, more than 350,000 articles carried a tag, suggesting widespread unverified claims. Recent advances in Graph Neural Networks (GNNs) open new opportunities to model Wikipedia not only through text but also through its citation structures. Traditional citation benchmarks (Cora, CiteSeer, PubMed) suffer from limited diversity [@yang2016revisiting; @shchur2018pitfalls], leading to the introduction of Wiki-CS, a richer Wikipedia-based dataset [@mernyei2020wikics]. Within this landscape, approaches can be divided into content-based, focusing on semantics and syntax, and context-based, emphasizing external signals such as social or citation networks [@montiFakeNewsDetection2019].

Our project adopts a context-based approach, an alternative to text-driven methods, by leveraging article relations (internal links) and article structure (sections, citations, length) [@hanGraphNeuralNetworks2020]. A central question here is whether Wikipedia’s internal links function similar to academic citations, where high-quality papers often cite other influential works [@arroyo2020science]. Unlike scholarly references, which carry signals of reliability and authority, Wikipedia’s internal links are often added freely whenever a related article exists. Many wikilinks are automatically generated or inserted via templates (infoboxes, navigation boxes, etc.) [@ruprechter2020relating]. By applying GNNs, we aim to explore to what extent signals of reliability and authority can propagate through these networks and how such context-based signals may complement structural features in predicting article quality. **\[please check this again guys, thy - Lorenz\]**

# Data

## Data Collection

This project combined three complementary data sources to capture different aspects of Wikipedia articles: the Pageviews API (popularity and user attention), the Edit History API (editorial activity patterns, including user and bot edits), and the Wikipedia API (article crawling and network construction). Article metadata was retrieved by mapping page IDs to titles via the MediaWiki API, which only supports up to 50 IDs per request; page IDs were split into batches of 50, queried in parallel with ThreadPoolExecutor, and merged back into the dataset before saving to CSV. One year of pageview data (July 2023–July 2024) was collected for each article from the Wikimedia REST API, aggregated into annual totals, and stored incrementally to prevent data loss; parallel requests and tqdm progress tracking ensured efficiency. Temporal metadata was added by retrieving last edit timestamps through the REST API’s `/page/summary/{title}` endpoint, using randomized delays (0.3–0.6s), retry logic for HTTP 429 errors, and parallel workers to accelerate processing. Editorial activity was captured for July 2023–July 2025, with edit counts broken down by registered users, anonymous users, group bots, and name bots, then aggregated into human vs. bot contributions. To respect API limits, randomized delays, proxy usage and periodic checkpoints were used; data collection was parallelized for efficiency. Together, these steps produced a comprehensive dataset covering article text and structure, popularity, recency, editorial activity, and network relationships, providing a robust foundation for downstream analyses.

\<\<\<\<\<\<\< HEAD The network was obtained by a BFS-search starting at a handful of seed articles (**exact number at least**) and then expanding this seed through following internal links until a sufficient network size was reached. Then only the internal links pointing to articles in the sample were kept. This gave a snowball sample of the whole of wikipedia. It has to be noted that this provides a biased sample of the network, that by no means is representative of all of wikipedia. A better approach would have been to follow the one outlined by [-@cristianconsonniWikiLinkGraphsCompleteLongitudinal2019]. ======= The network was obtained by a BFS-search starting with 4 seed articles (Influenza, Serena Williams, French Revolution, Quantum mechanics) and then expanding this seed through following internal links until a sufficient network size was reached. Each seed-article belongs to one of the four largest Wiki-Projects (Serena Williams → WikiProject Biography (largest), Influenza → WikiProject Medicine, French Revolution → WikiProject History, Quantum mechanics → WikiProject Physics). This was done to get a solid variety of article data. Only the internal links pointing to articles in the sample were kept. This gave a snowball sample of the whole of wikipedia. It has to be noted that this provides a biased sample of the network, that by no means is representative of all of wikipedia. A better approach would have been to follow the one outlined by [@cristianconsonniWikiLinkGraphsCompleteLongitudinal2019]. \>\>\>\>\>\>\> d6a1d9ebcaaa3add12c9391b611addbe222c833f

### Article Quality

Wikipedia articles are rated on an ordinal quality scale. In this project the following classes are used as the target: FA, FL, FM, A, GA, B, C, Start, Stub, List.

```{r}
library(knitr)
library(dplyr)

quality_classes <- tribble(
  ~Class, ~Meaning,
  "FA", "Featured Article – highest quality, comprehensive and well-sourced",
  "FL", "Featured List – best-quality lists, complete and well-referenced",
  "FM", "Featured Media – high-quality non-textual media (images, videos, etc.)",
  "A", "Near-featured quality, but may need minor improvements",
  "GA", "Good Article – accurate, well-structured, but less comprehensive than FA",
  "B", "Mostly complete, but still lacking references or polish",
  "C", "Useful coverage, but incomplete or missing important details",
  "Start", "Basic coverage, underdeveloped but beyond stub level",
  "Stub", "Very short or incomplete article, minimal information",
  "List", "Articles in list format, assessed on completeness and structure"
)

kable(quality_classes, caption = "Wikipedia quality assessment classes and their meaning")
```

## Dataset Analysis

### Article Features

A dataset of 379,926 English Wikipedia articles was assembled to support the quality prediction task. The collection combines article-level features, structural metadata, and editing history, enabling both content-independent and behavioral dimensions of quality to be examined. Articles span the full range of Wikipedia’s grading scheme, from Stub to Featured Article (FA), ensuring coverage of different writing styles, completeness levels, and editorial efforts. The design of this dataset is informed by prior research on text, structure, and verifiability [-@bassani2019quality], [-@reddy2021nwqm], [-@redi2019citation] as well as graph-based benchmarks such as Wiki-CS [-@mernyei2020wikics]. Drawing on these insights, the dataset integrates both article-level descriptors and network-oriented variables.

For each article, descriptive attributes include page length, number of references, number of sections, templates, infobox presence, and pageviews. Structural metadata records the number of categories, links, and depth in the category hierarchy. Editorial activity is tracked through detailed revision histories, separating human and bot edits and further distinguishing between registered, anonymous, and automated accounts. To reflect recent collaboration dynamics, edit-related variables were restricted to the past two years. Finally, additional context such as last edit timestamp, days since last edit, and protection status was included to capture recency and stability. This design results in a dataset that captures both structural and editorial signals, complementing traditional content-based features and enabling a multi-perspective analysis of Wikipedia article quality.

```{r}
library(knitr)
library(dplyr)

# Define variables, categories, and definitions
# Are these network features? i dont really get the categories? maybe check this again?
vars <- tribble(
  ~Category, ~Variable, ~Definition,
  "Structure","num_categories","Number of categories assigned to the article.",
  "Structure","num_links","Total number of internal/external links.",
  "Structure","page_length","Length of the article (characters).",
  "Structure","num_references","Number of citations in the article.",
  "Structure","num_sections","Number of sections.",
  "Structure","num_templates","Number of templates used.",
  "Structure","has_infobox_encoded","1 if an infobox exists, otherwise 0.",
  "Structure","protection_status_encoded","Encoded protection level.",
  "Style / Semantic","assessment_source_umap_1","UMAP dim 1 of assessment source.",
  "Style / Semantic","assessment_source_umap_2","UMAP dim 2 of assessment source.",
  "Style / Semantic","assessment_source_umap_3","UMAP dim 3 of assessment source.",
  "Network","days_since_last_edit","Days since the last edit.", 
  "Network","edits_all_types","Total edits (last two years).",
  "Network","edits_anonymous","Anonymous edits (last two years).",
  "Network","edits_bot","Bot edits (last two years).",
  "Network","edits_group_bot","Group-bot edits (last two years).",
  "Network","edits_human","Human edits (last two years).",
  "Network","edits_name_bot","Named-bot edits (last two years).",
  "Network","edits_user","Registered-user edits (last two years).",
  "Network","pageviews_Jul2023Jul2024","Pageviews from Jul 2023–Jul 2024."
)

# Render table
kable(vars, caption = "Variables grouped by category with their definitions")
```

### Article-Feature Analysis

The dataset comprises \~380,000 Wikipedia articles labeled with quality classes, but the distribution is highly imbalanced: most articles fall into low-quality categories (Stub, Start), while only a small minority reach high-quality levels (FA, GA, FL, A). Quality progression is evident—higher-quality articles are much longer and include richer structural and citation features such as references, links, and sections. In contrast, Stub and Start articles remain short and sparsely referenced, reflecting limited editorial development. These patterns confirm that structural richness and citation density are closely associated with editorial quality.

```{r}
# Feels like redundant information with the chart below

# library(tibble)
# library(dplyr)
# library(scales)
# library(knitr)

# tbl <- tribble(
#   ~`Quality Class`, ~Count, ~`Avg. Page Length`, ~`Avg. References`, ~`Avg. Links`, ~`Avg. Sections`,
#   "A",     113,    60096.51,  97.15, 386.11, 17.62,
#   "B",   29768,    61135.93,  96.63, 422.76, 21.02,
#   "C",   74983,    33138.79,  47.86, 285.05, 14.69,
#   "FA",   1582,    89048.49, 142.50, 514.55, 21.72,
#   "FL",    320,    58989.33,  87.65, 370.57, 11.29,
#   "GA",   5934,    64000.73, 115.96, 395.99, 17.88,
#   "List", 13161,   29490.59,  29.69, 368.69, 13.75,
#   "Start",162145,  14064.61,  17.70, 179.28,  8.25,
#   "Stub", 91920,    5878.57,   6.29, 143.61,  4.21
# ) |>
#   mutate(
#     Count = comma(Count),
#     `Avg. Page Length` = comma(`Avg. Page Length`, accuracy = 0.01),
#     `Avg. References`  = number(`Avg. References`,  accuracy = 0.01),
#     `Avg. Links`       = number(`Avg. Links`,       accuracy = 0.01),
#     `Avg. Sections`    = number(`Avg. Sections`,    accuracy = 0.01)
#   )

# kable(tbl, caption = "Quality classes and average structural metrics.")

```

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=5cm, keepaspectratio]{Images/quality_distribution.png}
    \caption{Wikipedia Graph - Degree Distribution and Power Law Analysis}
    \label{fig:pca_combined}
\end{figure}

The heatmap shows strong correlations among structural features, with the highest between page length and references (0.86), indicating that longer articles are usually better structured and more thoroughly referenced. Links are also positively correlated but provide partly independent information. A log–log scatter plot of links versus references confirms this trend: articles with more links often include more references, though variation remains, showing that links and references capture complementary aspects of article richness.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=5cm, keepaspectratio]{Images/features_corr.png}
    \caption{Wikipedia Graph - Degree Distribution and Power Law Analysis}
    \label{fig:pca_combined}
\end{figure}

In Feature Distributions plots, most articles cluster at the low end for page length, references, links, sections, pageviews, and recency of edits, with only a few outliers reaching extreme values—reflecting Wikipedia’s heterogeneity, where a small subset dominates in depth and attention..

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=5cm, keepaspectratio]{Images/features_dis.png}
    \caption{Wikipedia Graph - Degree Distribution and Power Law Analysis}
    \label{fig:pca_combined}
\end{figure}

Pageviews vary widely across classes. While Featured Articles (FA) and Good Articles (GA) generally attract higher median views, many B-class and even lower-quality articles also reach high visibility. This suggests that popularity is not fully aligned with editorial quality, articles can be widely read even if their structural quality is limited.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=5cm, keepaspectratio]{Images/pageview.png}
    \caption{Wikipedia Graph - Degree Distribution and Power Law Analysis}
    \label{fig:pca_combined}
\end{figure}

Feature Relationships Pairwise feature comparisons show clear clustering by quality: high-quality articles combine length, references, links, and sections in consistent proportions, while low-quality articles remain compact across all dimensions. Pageviews and recency of edits add further variation but only partially align with quality, reinforcing that structural completeness and editorial effort are the strongest signals of quality.

### Network Description

As already mentioned we create the network from the sampled articles using the internal links of wikipedia articles. This way we obtained a directed network with a single weakly connected component. The network is very sparse (Density = 0.0002), which is no surprise for a network of this size. There is also some clustering in the network and the graph has a small diameter (approximated). This make sense since the articles were obtained by BFS and collection did not go further than 4 steps. Notable is the high share of reciprocal relations which shows that many articles link each other.

```{r, echo=FALSE}
knitr::kable(data_descriptive, caption = "Network Descriptive Metrics", 
              digits = 5, 
              format.args = list(scientific = FALSE))
```

The degree distributions for the network are very long tailed, which is typical for many internet and citation networks. This can the consequence of the age of articles or some sort of preferential attachment or local redirection mechanism.\
We see some difference between In- and Out-Degree. In-degrees have a much higher range, up to 75k, while Out-degrees are much smaller, up to 2,5k. This is because an article can be linked to many more articles than it can link itself. Interestingly the means are very similar. The power-law is a marginally better fit for the In-degrees (smaller KS-distance) and $\alpha_{In}$ is lower, but still above 2.

\begin{figure}[H]
    \begin{subfigure}{0.5\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{Images/InDegree_PowerLaw.png}
        \label{fig:Number of Components}
    \end{subfigure}
    \hfill % This adds space between the two subfigures
    \begin{subfigure}{0.5\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{Images/OutDegree_PowerLaw.png}
        \label{fig:Biggest (weakly) Connected Component}
    \end{subfigure}
    \caption{Degree Distributions with Power Law Fits}
    \label{fig:pca_combined}
\end{figure}

For none of the tested attributes strong assortativity was measured, with different implications. For the degree there is no homophily or heterophily between articles of similar or dissimilar, in- or out-degrees. Showing that the network is not forming structures along these properties.

```{r, echo=FALSE}
knitr::kable(data_homophily, caption = "Network Assortativity", 
              digits = 4, 
              format.args = list(scientific = FALSE))
```

For the Article-Quality the network is also not showing notable assortativity. There are only marginal increases by changing the encoding of the varibles, either by aggregating categorical varibles or switching to a numerical encoding. Higher values would have proved that articles form homogenous communities based on their quality. This already strongly discourages the hypothesis that Quality-signals propagate directly through channels in the network and only leaves the option of other node-properties being connected to certain qualities. It also already foreshadows why the direct network structure does not provide much help in classifying the article-nodes, since many GNNs rely on homophily by aggregating information from the neighborhood of nodes.

### Network Features

To further enrich the dataset we use the wikipedia graph to create network based features for the article-nodes. The hope is that these will provide crucial additional information to help classifying them. For example it is conceiveable that certain article-qualities are associated with certain structural positions in the network.

```{R}
network_features_tbl <- tribble(
  ~`Feature Category`, ~`Feature Name`,
  "Degree Centrality", "In-Degree",
  "Degree Centrality", "Out-Degree", 
  "Local Structure", "Clustering Coefficient",
  "Path-based", "Betweenness Centrality",
  "Core Structure", "Coreness Centrality",
  "Link Analysis", "PageRank",
  "Link Analysis", "HITS Hub Score",
  "Link Analysis", "HITS Authority Score",
  "Reciprocity", "Share of Reciprocal Relations",
  "Spectral Features", "Spectral Embedding (9D)",
  "Spectral Features", "Spectral Modularity Row Sums",
  "Probabilistic Features", "Transition Probability Max"
)

kable(network_features_tbl, caption = "Network-based features extracted from Wikipedia Graph")
```

In the following plots we have separated the articles by their quality. For consistency, the observed trend should preserve the quality-category order (HQ-MQ-LQ). The plotted values are pre-quantile-scaling as that would have greatly hampered their interpretability.

For In-Degree we see the very low share of very high in-degree articles, mainly in the MQ category. On the lower end of the in-degrees ($<5000$) we see that HQ articles have a higher share of articles with increasing in-degree. For the Out-Degree this is much more visible, showing that higher quality articles tend to link to more articles.

The clustering shows that higher quality articles tend to be less clustered, so more functioning as hubs, inhabiting bridging positions, also when having more conenctions share of relations between neighbors rises much slower. Which is also supported by the relatively higher Betweenness centrality. Here the Coreness Centrality somehow speaks of a different picture, showing higher quality networks are more deeply embedded into the network.

For Page-Rank and both HITS centralities (Hubs and Authority) we see clearly, at least for the Hubs-Score, that higher Quality articles posses higher scores. Unfortunatly this becomes less clear for Authority and Pagerank scores, because of the width of the distributions, showing that a large majority of all articles posses very low scores, but the general trend (HQ\>MQ\>LQ) held. Here the sampling method might have produced these exceptionally high values, though this is also conformed by the other node-level measures.

A very interesting case is the share of reciprocal relations. Here we see for HQ and MQ higher shares of low reciprocity and lower shares of high reciprocity. Interestingly LQ articles show a very steady almost linear decline. Which also fits well with the In- and Out-degree results.

All these results point towards a qualitative difference in node-properties relating to article quality and thus should provide helpful information for classification.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=5cm, keepaspectratio]{Images/netfeatures_category.png}
    \caption{CCDF for Network Features of Wikipedia Graph}
    \label{fig:pca_combined}
\end{figure}

Furthermore we created a 9-Dimensional Spectral Embedding for the graph and used it as features for the dataset. Spectral embedding maps nodes to a low-dimensional space using eigenvectors of graph matrices (like the Laplacian), where the geometric distances preserve the graph's structural relationships. The embedding for the 2-D plot was chosen by the largest eigenvalue gap. There is some notable separation over the space, notably along 3-lines. Here probably also the sample structure played a major role in shaping this feature. In the plot we can also see that certain areas are more densely populated by either green or blue, which could be taken as a sign that certain article qualities inhabit a different structural position in the network. This would also support our earlier findings for the other network-features.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=5cm, keepaspectratio]{Images/2dspectral.png}
    \caption{2D Spectral Node Embedding produce by Wikipedia Graph Sample}
    \label{fig:pca_combined}
\end{figure}

## Preprocessing

During the pre-processing we face two important challenges. The first was the highly imbalanced target variable with only a very small fraction of high-quality articles. The second challenge were the heavily skewed distributions, especially for the graph based features. These have to be adressed to get

The dataset was prepared for modeling by constructing target labels and encoding structured features. Articles were indexed by title, page ID, and numeric identifiers for efficient lookup. Each article was mapped to its Wikipedia quality class (FA, GA, B, etc.), from which three target variables were derived: a 10-level ordinal scale (`Target_QC_cat`), a 3-tier aggregate scale (`Target_QC_aggcat`) (High Quality (HQ) \> Medium Quality (MQ) \> Low Quality (LQ) ), and a log-transformed numeric variant (`Target_QC_numlog`).

Categorical and binary article-attributes were encoded, including protection status (integer labels), infobox presence (binary), and assessment source (one-hot, then reduced with UMAP). The final feature set integrated content metrics (page length, sections, templates, references, categories, links), editorial activity (days since last edit, human vs. bot edits), and popularity (annual pageviews, July 2023–July 2024). Together, these features capture structural, editorial, and popularity dimensions of Wikipedia articles, providing information for the models.

The features had to be preprocessed since the numerical ranges for many features, especially the network features, were very concentrated to a small range and thus not fit for training. Here normalization proved to be insufficient. The distributions particularly for the network metrics are immensely positvely skewed. Different scaling methods such as standard, minmax, robust and robust-log scaling proved to be ineffective to generate reasonably spread distributions. The only approach that brought reasonable results was quantile scaling. Quantile scaling (n=500) transforms data by mapping each value to its percentile rank, creating a uniform distribution where extreme outliers get compressed while preserving relative order.

From the wikipedia graphs we removed nodes with total $degree(k) ≤ 1$.

An extra dataset with only article-features was created to compare if using the network features provided helpful information.

# Methods

## Benchmarks

To assess the added value of our graph-based models, we implemented several benchmarks: a Random Forest classifier, the ORES quality prediction service, and two Multilayer Perceptrons (MLPs).

### Random Forest

We train on the preprocessed tabular dataset (scaled_data_quantile_Target_QC_aggcat.parquet) with the aggregated three-class quality target. The feature table combines article-level attributes (e.g. length/structure/citations) and network-derived measures (e.g., centrality/degree-style features). For the Random Forest we use a fixed configuration (e.g. n_estimators=200, constrained depth, max_features='sqrt', bootstrap), fit on the training data (with validation used during development), and report accuracy, precision, recall, F1, and confusion matrices on the test set. We also assess permutation importances to gauge feature contributions.

### ORES (Objective Revision Evaluation Service)

We query the Wikimedia ORES article-quality model for the same test articles, using their revision IDs. ORES six-way predictions are mapped to the project’s three aggregated classes for a like-for-like comparison. We then compute the same metrics as above and store detailed predictions for inspection. This positions our models against the operational standard in the Wikipedia ecosystem.

### **Multilayer Perceptrons (MLPs)**

-   Full Features

-   Non-network Features

The baseline model is implemented as a Multi-Layer Perceptron (MLP) that relies solely on node features, without incorporating any graph structural information. This design allows the model to serve as a benchmark to evaluate the added value of graph-based architectures. The MLP applies input normalization, a linear projection, and GELU activation, followed by three fully connected hidden layers. Each hidden layer is equipped with LayerNorm, residual connections, and adaptive dropout, which together enhance stability and generalization. The output is produced through a final normalization and projection layer.

The optimized configuration specifies a hidden dimension of 128, three layers in total, and a very small dropout rate of approximately 0.014, suggesting that the model benefits from retaining nearly all feature information. Training is performed with a learning rate of 0.00144 and a weight decay of 2.46×10\^-4. To handle the class imbalance in the dataset, Class-Balanced Focal Loss is employed with β = 0.9999 and γ = 2.55. Additionally, boosted oversampling is applied, increasing minority class representation by a factor of about five, with a minimum samples factor of four. Under these optimized settings, the MLP baseline reached a validation score of 0.805, providing a solid reference point for comparison with graph neural network models.

## Graph-Neural-Network Models

### Graph-Convolutional

-   Improved GNN

    The Improved GNN integrates multiple architectural enhancements to strengthen learning on imbalanced graph data. The model begins with input preprocessing using LayerNorm and a linear projection into the hidden dimension. It applies three layers of graph convolutions, configurable as either GraphSAGE or GCN, each followed by LayerNorm, GELU activation, and adaptive dropout that decreases in later layers for stability. Weighted residual connections are employed, with residual strength gradually increasing in deeper layers to maintain gradient flow and feature reuse. The final stage includes an output normalization and a projection layer for prediction. Optimization is performed with a learning rate of 0.0173 and weight decay of 4.2×10\^-6. The training objective leverages Class-Balanced Focal Loss with parameters β = 0.9993 and γ = 3.13, effectively addressing class imbalance. This is further reinforced by boosted oversampling with a minimum class boost of approximately 4.84 and a min_samples_factor of 4.

-   Residual GCN

    This model employs a residual Graph Convolutional Network (GCN) architecture enhanced with modern design choices for stability and improved performance on imbalanced graph data. The input features are projected into a hidden dimension using a linear transformation followed by GELU activation. Two GCNConv layers are applied sequentially, each followed by LayerNorm, GELU activation, and dropout. Residual connections are incorporated with a weighting factor of α = 0.9, which prioritizes newly transformed representations while still retaining prior information to ensure gradient stability. A final linear projection produces the classification outputs. Optimization is performed with AdamW using a learning rate of 0.00173 and a weight decay of 2.95×10\^-5. To handle class imbalance, Focal Loss is applied with α = 1.11 and γ = 3.95, together with a boosted oversampling strategy that increases minority class representation by a factor of four.

### Graph-Sage

The model is based on a residual version of GraphSAGE, designed to enhance stability and performance on imbalanced graph data. The architecture begins with an input projection that embeds the raw node features into a hidden dimension, followed by two SAGEConv layers. Each layer applies LayerNorm, GELU activation, and dropout to improve expressiveness and regularization. Residual connections are incorporated with a weighted coefficient α = 0.85, allowing a balance between new transformed features and the identity mapping from the previous layer. A final linear layer produces the output predictions. Optimization uses AdamW with a learning rate of 0.00128 and weight decay of 0.00729. To address class imbalance, Focal Loss is applied with α = 1.00 and γ = 3.98, combined with a balanced oversampling strategy that increases minority class representation by a factor of four.

### Graph Attention

It begins with the input features and passes them through three stacked GATConv layers, each followed by LayerNorm to stabilize training. The attention mechanism employs two heads, and the final layer averages their outputs to produce predictions. ELU activations and a dropout rate of approximately 0.64 are applied to enhance generalization and reduce overfitting. Training is carried out with AdamW optimization, using a learning rate of 0.00043 and a weight decay of 7.5×10\^-5. To address class imbalance, Focal Loss is employed with parameters α = 2.83 and γ = 3.78, while boosted oversampling is applied to increase minority class representation with a factor of two.

## Training and Evaluation

A range of different models was chosen for evaluation. With these candidates we performed a hyperparameter optimization over a limited search space. For each model 16 variants were created. Then started the finals runs where we used 3-fold cross-validation on different splits for each model.

\begin{figure}[H]
    \begin{subfigure}{0.3\textwidth} 
        \centering
        \includegraphics[width=\textwidth]{Images/enhanced_improved_gnn_data_quantile_Target_QC_aggcat_run1.png} %Path to images 
        \label{fig:Multilayer Perceptron}
        \caption{GCN} %Visible Caption
    \end{subfigure}
    \hfill 
    \begin{subfigure}{0.3\textwidth} 
        \centering
        \includegraphics[width=\textwidth]{Images/enhanced_residual_gcn_data_quantile_Target_QC_aggcat_run1.png}
        \label{fig:Concolutional Graph Neural Network}
        \caption{Residual GCN} %Visible Caption
    \end{subfigure}
    \hfill 
    \begin{subfigure}{0.3\textwidth} 
        \centering
        \includegraphics[width=\textwidth]{Images/enhanced_residual_sage_data_quantile_Target_QC_aggcat_run1.png}
        \label{fig:GraphSAGE Model} %Invisible! 
        \caption{Residual Sage} %Visible Caption
    \end{subfigure}
    \caption{Test and Validation Loss and Accuracy during Training I} %Visible Caption 
    \label{fig:}
\end{figure}

The dataset was partitioned using two different strategies depending on the experimental phase. For hyperparameter optimization, a 70/15/15 (train/validation/test) split was employed on a single random partition. For final model evaluation, we used an 80/10/10 split across three different random seeds (42, 142, 242) to assess model stability. This approach follows graph-aware splitting principles, preserving the connectivity structure essential for GNN performance.

\begin{figure}[H]
    \begin{subfigure}{0.3\textwidth} 
        \centering
        \includegraphics[width=\textwidth]{Images/enhanced_gat_data_quantile_Target_QC_aggcat_run1.png} %Path to images 
        \label{fig:Multilayer Perceptron}
        \caption{GAT} %Visible Caption
    \end{subfigure}
    \hfill 
    \begin{subfigure}{0.3\textwidth} 
        \centering
        \includegraphics[width=\textwidth]{Images/enhanced_mlp_data_quantile_Target_QC_aggcat_run1.png}
        \label{fig:Concolutional Graph Neural Network}
        \caption{MLP Network- and Article-Features} %Visible Caption
    \end{subfigure}
    \hfill 
    \begin{subfigure}{0.3\textwidth} 
        \centering
        \includegraphics[width=\textwidth]{Images/enhanced_mlp_data_nonnetwork_quantile_Target_QC_aggcat_run1.png}
        \label{fig:GraphSAGE Model} %Invisible! 
        \caption{MLP Article-Features} %Visible Caption
    \end{subfigure}
    \caption{Test and Validation Loss and Accuracy during Training II} %Visible Caption 
    \label{fig:}
\end{figure}

Graph neural networks (GCN, ResidualGCN, ResidualSAGE, GAT) show varying convergence patterns, with some models achieving faster validation accuracy improvements than others. There seems to be a lot of overfitting to training data, especially for the GAT where we dont see much change in validation loss and accuracy, while the training scores are increasing.

### Hyperparameter Optimization

Systematic hyperparameter optimization was conducted using Optuna with Tree-structured Parzen Estimator (TPE) sampling. The search space encompassed universal parameters including dropout rates (0.01-0.7), weight decay (1e-6 to 1e-2 on log scale), and learning rates (1e-4 to 1e-1 for most models, 1e-4 to 1e-2 for GAT). Model architecture parameters included hidden dimensions selected from \[32, 64, 128, 256\] and layer counts ranging from 2-5 (2-3 for GAT due to stability concerns). Model-specific parameters such as GAT attention heads \[2,4,8,16\] and loss function selection were also optimized.

The optimization objective combined 60% minority class F1-score with 40% overall accuracy, emphasizing performance on underrepresented high-quality articles while maintaining overall predictive capability. Each model underwent 16 trial, with 40 epochs per trial during optimization and 80-200 epochs for final evaluations.

### Model Training and Validation

Given the severe class imbalance in Wikipedia article quality ratings, multiple strategies were implemented to address this challenge. We experimented with specialized loss functions including Focal Loss (α=1.0-3.0, γ=2.0-4.0), Class-Balanced Focal Loss (β=0.999-0.9999), and Weighted Cross-Entropy.

Additionally oversampling strategies were employed: "balanced" sampling ensuring equal representation per class, and "boosted" sampling with aggressive minority class enhancement (2.0-5.0x factors). To maintain computational feasibility, memory-safe sampling was limited to 20,000 samples maximum.

All models employed early stopping with patience values between 25-50 epochs and minimum delta thresholds of 1e-4 to prevent overfitting. Optimization utilized the Adam optimizer with OneCycleLR scheduling, featuring cosine annealing and 10% warmup periods. Regularization techniques included gradient clipping (max_norm=1.0) and model-specific dropout rates. Because of memory limitation, GraphSAINT subgraph sampling was implemented with batch sizes of 2048-8192, walk lengths of 2, and 5-8 sampling steps per epoch.

Model performance was assessed using standard classification metrics including accuracy, precision, recall, and F1-scores (macro because of class imbalance). Special attention was paid to minority class performance, given the practical importance of correctly identifying high-quality articles. Cross-validation results report mean performance with standard deviations across runs, providing confidence intervals for model comparisons. The MLP baselines, both with and without network features, provide important performance benchmarks for assessing the value of graph structure.

# Results

Our evaluation covered multiple GNN architectures, traditional ML baselines, and the ORES system. The cross-validation results show clear performance differences across approaches.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=10cm, keepaspectratio]{Images/final_plot.png}
    \caption{Cross-validation results for all models with error bars showing standard deviation across runs}
    \label{fig:cv_results}
\end{figure}

## Model Performance

Residual GraphSAGE achieved the highest accuracy at 80.50%, followed closely by Random Forest at 79.57%. The GNN approaches showed mixed results - while Residual GCN reached 75.62% and standard GCN got 74.45%, GAT performed poorly at only 39.39%. This GAT failure likely stems from the networks lack of homophily, which we observed in our assortativity analysis. GAT's attention mechanism needs meaningful neighbor relationships, but our network structure doesn't provide this.

Comparing the MLP variants and investigating the importance of features used by the Random Forest reveals that network features matter: the full-feature model (75.26%) outperformed the article-only version (71.80%) by 3.46 percentage points. All our models beat the ORES baseline (64.12%) by substantial margins.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=10cm, keepaspectratio]{Images/accuracy_comparison_final.png}
    \caption{Final accuracy comparison across all evaluated models}
    \label{fig:accuracy_comparison}
\end{figure}

## Which Features matter?

The Random Forest analysis lets us gauge which features drive quality predictions. Content structure dominates - page_length (18.51%), num_references (12.67%) and num_sections (7.78%) are the top predictors.

Network features also contribute meaningfully: degree_out_centrality (3.40%), transition_max_prob (3.13%), and hub scores (2.45%) all rank in the top 10. The spectral embeddings capture additional structural patterns that explicit network metrics miss. Editorial activity like total edits (2.76%) provides another quality signal.

The permutation importance results largely agree with Gini importance for the top features, confirming page_length and num_references as robust predictors across different measurement approaches.

## Cross-Validation Patterns

Some models showed zero variance across runs (Residual GCN, GAT, Residual SAGE), while others had healthy variance around 0.4-0.6%. The zero-variance cases might indicate very stable convergence or potential issues with local optima.

Class imbalance handling varied significantly. Random Forest achieved the best macro F1-score (62.04%), while neural approaches clustered around 48-53%. This suggests the tree-based approach handles minority classes better than the neural networks, despite the various loss functions and sampling strategies we tried.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=8cm, keepaspectratio]{Images/Averaged_Confusion_Matrices.png}
    \caption{Averaged confusion matrices across cross-validation runs for all models [NEED THE ALL CONFUSION MATRICES PNG HERE - Lorenz]}
    \label{fig:confusion_matrices}
\end{figure}

Residual SAGE's success over other GNNs makes sense given our network analysis. Its sampling approach works better in sparse, heterophilic networks where traditional message-passing assumptions break down. Rather than assuming similar nodes cluster together, SAGE can aggregate from diverse neighborhoods, which fits Wikipedia's link structure better.

# Discussion and Conclusion

(Performance Comparison, Interpret the results in the context of social network theory, Key Findings and Implications)

-   Sampling method biased, alternative whole network approach

-   imbalanced classes with very small HQ class

-   more extensive testing of architectures and parameters

-   missing assortativity/homophily -\> no direct quality propagation (providing network structure did not meaningfully improve GNNs over MLP)

-   some hints for network structure and position being related article quality (MLP performed better), which by a few percentage points is already remarkable, as these have to be stacked in order to provide meaningful improvements

-   The feature importance seen in the RF results reflect a broader characteristic of the underlying Wikipedia link graph. Unlike academic citations - internal Wikipedia links are added mainly for navigation and cohesion rather than endorsement. Consequently, link-based features alone do not strongly predict article quality, though an article’s position in the network can still carry valuable information. This distinction between links and citations underlines why structural features remain dominant, while network features add only complementary value to our quality assessment models.

# References

## Code and Data \[Whats supposed to go here? - Lorenz\]

-   API´s
-   Python Packages

## Literature