@article{cristianconsonniWikiLinkGraphsCompleteLongitudinal2019,
  title = {{{WikiLinkGraphs}}: {{A}} Complete, Longitudinal and Multi-Language Dataset of the {{Wikipedia}} Link Networks},
  author = {{Cristian Consonni} and {David Laniado} and {A. Montresor}},
  year = {2019},
  journal = {International Conference on Web and Social Media},
  doi = {10.1609/icwsm.v13i01.3257},
  abstract = {Wikipedia articles contain multiple links connecting a subject to other pages of the encyclopedia. In Wikipedia parlance, these links are called internal links or wikilinks. We present a complete dataset of the network of internal Wikipedia links for the 9 largest language editions. The dataset contains yearly snapshots of the network and spans 17 years, from the creation of Wikipedia in 2001 to March 1st, 2018. While previous work has mostly focused on the complete hyperlink graph which includes also links automatically generated by templates, we parsed each revision of each article to track links appearing in the main text. In this way we obtained a cleaner network, discarding more than half of the links and representing all and only the links intentionally added by editors. We describe in detail how the Wikipedia dumps have been processed and the challenges we have encountered, including the need to handle special pages such as redirects, i.e., alternative article titles. We present descriptive statistics of several snapshots of this network. Finally, we propose several research opportunities that can be explored using this new dataset.},
  annotation = {ARXIV\_ID: 1902.04298\\
MAG ID: 2954330497\\
S2ID: 1bbeb28b10b99f9280a29e16f6ba04110cbd5a67},
  file = {/Users/ramius/Zotero/storage/MTUTJC96/Cristian Consonni et al. - 2019 - WikiLinkGraphs A complete, longitudinal and multi-language dataset of the Wikipedia link networks.pdf}
}

@article{edisonmarrese-taylorEditcentricApproachWikipedia2019,
  title = {An {{Edit-centric Approach}} for {{Wikipedia Article Quality Assessment}}.},
  author = {{Edison Marrese-Taylor} and {Marrese-Taylor}, Edison and {P. Loyola} and Loyola, Pablo and Matsuo, Yutaka and {Yutaka Matsuo} and Matsuo, Yutaka},
  year = {2019},
  month = nov,
  journal = {Conference on Empirical Methods in Natural Language Processing},
  pages = {381--386},
  doi = {10.18653/v1/d19-5550},
  abstract = {We propose an edit-centric approach to assess Wikipedia article quality as a complementary alternative to current full document-based techniques. Our model consists of a main classifier equipped with an auxiliary generative module which, for a given edit, jointly provides an estimation of its quality and generates a description in natural language. We performed an empirical study to assess the feasibility of the proposed model and its cost-effectiveness in terms of data and quality requirements.},
  annotation = {ARXIV\_ID: 1909.08880\\
MAG ID: 2985332770\\
S2ID: 1ec28bdcacf6fd2c72c227ccd74a2ad1ee8d4b1b},
  file = {/Users/ramius/Zotero/storage/B3ULS8YH/Edison Marrese-Taylor et al. - 2019 - An Edit-centric Approach for Wikipedia Article Quality Assessment..pdf}
}

@misc{hanGraphNeuralNetworks2020,
  title = {Graph {{Neural Networks}} with {{Continual Learning}} for {{Fake News Detection}} from {{Social Media}}},
  author = {Han, Yi and Karunasekera, Shanika and Leckie, Christopher},
  year = {2020},
  month = aug,
  number = {arXiv:2007.03316},
  eprint = {2007.03316},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2007.03316},
  urldate = {2025-08-02},
  abstract = {Although significant effort has been applied to fact-checking, the prevalence of fake news over social media, which has profound impact on justice, public trust and our society as a whole, remains a serious problem. In this work, we focus on propagation-based fake news detection, as recent studies have demonstrated that fake news and real news spread differently online. Specifically, considering the capability of graph neural networks (GNNs) in dealing with non-Euclidean data, we use GNNs to differentiate between the propagation patterns of fake and real news on social media. In particular, we concentrate on two questions: (1) Without relying on any text information, e.g., tweet content, replies and user descriptions, how accurately can GNNs identify fake news? Machine learning models are known to be vulnerable to adversarial attacks, and avoiding the dependence on text-based features can make the model less susceptible to the manipulation of advanced fake news fabricators. (2) How to deal with new, unseen data? In other words, how does a GNN trained on a given dataset perform on a new and potentially vastly different dataset? If it achieves unsatisfactory performance, how do we solve the problem without re-training the model on the entire data from scratch, which would become prohibitively expensive in practice as the data volumes grow? We study the above questions on two datasets with thousands of labelled news items, and our results show that: (1) GNNs can achieve comparable or superior performance without any text information to state-of-the-art methods. (2) GNNs trained on a given dataset may perform poorly on new, unseen data, and direct incremental training cannot solve the problem---this issue has not been addressed in the previous work that applies GNNs for fake news detection. In order to solve the problem, we propose a method that achieves balanced performance on both existing and new datasets, by using techniques from continual learning to train GNNs incrementally.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/Users/ramius/Zotero/storage/Q9TUP5YC/Han et al. - 2020 - Graph Neural Networks with Continual Learning for Fake News Detection from Social Media.pdf}
}

@misc{montiFakeNewsDetection2019,
  title = {Fake {{News Detection}} on {{Social Media}} Using {{Geometric Deep Learning}}},
  author = {Monti, Federico and Frasca, Fabrizio and Eynard, Davide and Mannion, Damon and Bronstein, Michael M.},
  year = {2019},
  month = feb,
  number = {arXiv:1902.06673},
  eprint = {1902.06673},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1902.06673},
  urldate = {2025-08-02},
  abstract = {Social media are nowadays one of the main news sources for millions of people around the globe due to their low cost, easy access, and rapid dissemination. This however comes at the cost of dubious trustworthiness and significant risk of exposure to `fake news', intentionally written to mislead the readers. Automatically detecting fake news poses challenges that defy existing content-based analysis approaches. One of the main reasons is that often the interpretation of the news requires the knowledge of political or social context or `common sense', which current natural language processing algorithms are still missing. Recent studies have empirically shown that fake and real news spread differently on social media, forming propagation patterns that could be harnessed for the automatic fake news detection. Propagation-based approaches have multiple advantages compared to their content-based counterparts, among which is language independence and better resilience to adversarial attacks. In this paper, we show a novel automatic fake news detection model based on geometric deep learning. The underlying core algorithms are a generalization of classical convolutional neural networks to graphs, allowing the fusion of heterogeneous data such as content, user profile and activity, social graph, and news propagation. Our model was trained and tested on news stories, verified by professional fact-checking organizations, that were spread on Twitter. Our experiments indicate that social network structure and propagation are important features allowing highly accurate (92.7\% ROC AUC) fake news detection. Second, we observe that fake news can be reliably detected at an early stage, after just a few hours of propagation. Third, we test the aging of our model on training and testing data separated in time. Our results point to the promise of propagation-based approaches for fake news detection as an alternative or complementary strategy to content-based approaches.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/ramius/Zotero/storage/5GR6C37M/Monti et al. - 2019 - Fake News Detection on Social Media using Geometric Deep Learning.pdf}
}

@article{tahayasseriDynamicsConflictsWikipedia2012,
  title = {Dynamics of Conflicts in {{Wikipedia}}.},
  author = {{Taha Yasseri} and Yasseri, Taha and {R{\'o}bert Sumi} and Sumi, R{\'o}bert and {Andr{\'a}s Rung} and Rung, Andr{\'a}s and {Andr{\'a}s Kornai} and Kornai, Andr{\'a}s and {J{\'a}nos Kert{\'e}sz} and Kert{\'e}sz, J{\'a}nos},
  year = {2012},
  month = jun,
  journal = {PLOS ONE},
  volume = {7},
  number = {6},
  doi = {10.1371/journal.pone.0038869},
  abstract = {In this work we study the dynamical features of editorial wars in Wikipedia (WP). Based on our previously established algorithm, we build up samples of controversial and peaceful articles and analyze the temporal characteristics of the activity in these samples. On short time scales, we show that there is a clear correspondence between conflict and burstiness of activity patterns, and that memory effects play an important role in controversies. On long time scales, we identify three distinct developmental patterns for the overall behavior of the articles. We are able to distinguish cases eventually leading to consensus from those cases where a compromise is far from achievable. Finally, we analyze discussion networks and conclude that edit wars are mainly fought by few editors only.},
  pmcid = {3380063},
  pmid = {22745683},
  annotation = {MAG ID: 2043253351},
  file = {/Users/ramius/Zotero/storage/4UAGSFT5/Taha Yasseri et al. - 2012 - Dynamics of conflicts in Wikipedia..pdf}
}

@article{thorstenruprechterRelatingWikipediaArticle2020,
  title = {Relating {{Wikipedia}} Article Quality to Edit Behavior and Link Structure},
  author = {{Thorsten Ruprechter} and Ruprechter, Thorsten and {Tiago Santos} and Santos, Tiago and {Denis Heli{\'c}} and Helic, Denis},
  year = {2020},
  journal = {Applied Network Science},
  volume = {5},
  number = {1},
  pages = {1--20},
  doi = {10.1007/s41109-020-00305-y},
  abstract = {Currently, the relation between edit behavior, link structure, and article quality is not well-understood in our community, notwithstanding that this relationship may facilitate editing processes and content quality on Wikipedia. To shed light on this complex relation, we classify article edits and perform an in-depth analysis of editing sequences for 4941 articles. Additionally, we build a network of internal Wikipedia hyperlinks between articles. Using this data, we compute parsimonious metrics to quantify editing and linking behavior. Our analysis unveils that conflicted articles differ substantially from others in almost all metrics, while we also detect slight trends for high-quality articles. With our network analysis we find evidence indicating that controversial and edit war articles frequently span structural holes in the Wikipedia network. Finally, in a prediction experiment we demonstrate the usefulness of edit behavior patterns and network properties in predicting conflict and article quality. With our work, we assist online collaboration communities, especially Wikipedia, in long-term improvement of content quality by offering valuable insights about the interplay of article quality, controversies and edit wars, editing behavior, and network properties via sequence-based edit and network-based article metrics.},
  annotation = {MAG ID: 3085747564\\
S2ID: d2742c2e19966865ab0db00fa515c79678eec9dd},
  file = {/Users/ramius/Zotero/storage/Q7WQFYAT/Thorsten Ruprechter et al. - 2020 - Relating Wikipedia article quality to edit behavior and link structure.pdf}
}

@article{yonglinWisdomCrowdsEffect2020,
  title = {Wisdom of Crowds: The Effect of Participant Composition and Contribution Behavior on {{Wikipedia}} Article Quality},
  author = {{Yong Lin} and Lin, Yan and {Chenxi Wang} and Wang, Chenxi},
  year = {2020},
  month = jan,
  journal = {Journal of Knowledge Management},
  volume = {24},
  number = {2},
  pages = {324--345},
  doi = {10.1108/jkm-08-2019-0416},
  abstract = {This paper aims to explore the effect of participant composition and contribution behavior of the different types of participants on the quality of knowledge generation in online communities.,This study samples all the featured articles in Chinese Wikipedia and performs a Cox regression to reveal how participant composition and contribution behavior affect the quality of articles in different contexts.,The results show that an increase in the number of participants increases the possibility of either enhancing or reducing the article quality. In most cases, the greater the proportion of core members (people who frequently participate in editing), the higher the possibility of enhancing the article quality. Occasional participants' editorial behavior hinders quality promotion, this negative effect weakens when such editorial behavior becomes more frequent.,The findings help to better leverage the role of online communities in practice and to achieve knowledge collaboration in a more efficient manner. For example, an appropriate centralized organizational form should be established in online communities to improve the efficiency of crowd contributions. And it is worth developing mechanism to encourage participants to frequently participate in editing the article.,This study contributes to the research on the organizational forms of online communities by showing the effect of participant composition and behavior in the new form of organizing on knowledge generation. This study also contributes to the research on wisdom of crowds by revealing who in a group of participants, in what context, and by what means influence knowledge generation.},
  annotation = {MAG ID: 3004113109\\
S2ID: da5790edd4224160d1da8107ba083736c9b8683e}
}
